{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\handw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\handw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\handw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "print(\"Gensim imported successfully\")\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_lstm.h5\")\n",
    "\n",
    "# Step 2: Load the Tokenizer\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Step 3: Define Sequence Length\n",
    "# Use the same sequence length you used during training\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Keren banget videonya! Penjelasannya jelas dan mudah dimengerti. Ditunggu konten selanjutnya, semangat terus!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Keren banget videonya! Penjelasannya jelas dan mudah dimengerti. Ditunggu konten selanjutnya, semangat terus!\n",
      "Processed Text: keren banget videonya penjelasannya mudah dimengerti ditunggu konten semangat\n"
     ]
    }
   ],
   "source": [
    "def load_slang_txt(file_path):\n",
    "    slang_dict_txt = {}\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            slang_dict_txt = json.loads(file_content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON in the file: {file_path}\")\n",
    "    return slang_dict_txt\n",
    "\n",
    "def load_slang_csv(file_path):\n",
    "    slang_df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    return dict(zip(slang_df.iloc[:, 0], slang_df.iloc[:, 1]))\n",
    "\n",
    "# Combine Slang Dictionaries\n",
    "slang_txt_path = 'combined_slang_words.txt'\n",
    "slang_dict_txt = load_slang_txt(slang_txt_path)\n",
    "\n",
    "slang_csv_path = 'new_kamusalay.csv'\n",
    "slang_dict_csv = load_slang_csv(slang_csv_path)\n",
    "\n",
    "slang_dict_tambahan = {\n",
    "    \"gw\": \"saya\", \"mau\": \"ingin\", \"ni\": \"ini\", \"aja\": \"saja\", \"gak\": \"tidak\", \"bgt\": \"sangat\",\n",
    "    \"klo\": \"kalau\", \"bgs\": \"bagus\", \"masi\": \"masih\", \"msh\": \"masih\", \"lom\": \"belum\",\n",
    "    \"blm\": \"belum\", \"ap\": \"apa\", \"brg\": \"barang\", \"ad\": \"ada\", \"blom\": \"belum\",\n",
    "    \"kebli\": \"kebeli\", \"tp\": \"tapi\", \"org\": \"orang\", \"tdk\": \"tidak\", \"yg\": \"yang\",\n",
    "    \"kalo\": \"kalau\", \"sy\": \"saya\", \"bng\": \"abang\", \"bg\": \"abang\", \"fto\": \"foto\",\n",
    "    \"spek\": \"spesifikasi\", \"cm\": \"cuma\", \"jg\": \"juga\", \"pd\": \"pada\", \"skrg\": \"sekarang\",\n",
    "    \"ga\": \"tidak\", \"gk\": \"tidak\", \"batre\": \"baterai\", \"gue\": \"saya\", \"dpt\": \"dapat\",\n",
    "    \"kek\": \"seperti\", \"mna\": \"mana\", \"mnding\": \"mending\", \"mend\": \"mending\",\n",
    "    \"dr\": \"dari\", \"sma\": \"sama\", \"drpada\": \"daripada\"\n",
    "}\n",
    "\n",
    "slang_dict = {**slang_dict_tambahan, **slang_dict_txt, **slang_dict_csv}\n",
    "\n",
    "# Stopwords (Adjusted)\n",
    "stpwds_id = list(set(stopwords.words('indonesian')))\n",
    "retain_words = ['baru', 'lama', 'sama', 'tapi', 'tidak', 'dari', 'belum', 'bagi', 'mau', 'masalah']\n",
    "for word in retain_words:\n",
    "    if word in stpwds_id:\n",
    "        stpwds_id.remove(word)\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to Replace Slang\n",
    "def replace_slang_in_text(text, slang_dict):\n",
    "    words = text.split()\n",
    "    replaced_words = [slang_dict.get(word, word) for word in words]\n",
    "    return ' '.join(replaced_words)\n",
    "\n",
    "# Informal Phrases\n",
    "informal_phrases = {\n",
    "    \"sat set sat set\": \"cepat\", \"ya mas\": \"\"\n",
    "}\n",
    "\n",
    "# Preprocessing Function\n",
    "def text_preprocessing(text, slang_dict):\n",
    "    # Case folding (convert text to lowercase)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove mentions, hashtags, and newlines\n",
    "    text = re.sub(r\"@[\\w]+|#[\\w]+|\\n\", \" \", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \" \", text)\n",
    "\n",
    "    # Remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub(r\"[^\\w\\s']\", \" \", text)\n",
    "\n",
    "    # Replace informal phrases\n",
    "    for phrase, replacement in informal_phrases.items():\n",
    "        text = text.replace(phrase, replacement)\n",
    "\n",
    "    # Replace slang terms\n",
    "    text = replace_slang_in_text(text, slang_dict)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stpwds_id]\n",
    "\n",
    "    # Lemmatization (optional, but can improve performance)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Stemming with exceptions\n",
    "    stemming_exceptions = {\"terasa\": \"terasa\", \"sat\": \"cepat\", \"set\": \"cepat\"}\n",
    "    tokens = [stemming_exceptions.get(word, word) for word in tokens]\n",
    "\n",
    "    # Reassemble the text and remove duplicates\n",
    "    text = ' '.join(dict.fromkeys(tokens))\n",
    "\n",
    "    return text\n",
    "\n",
    "# Process the Input Text\n",
    "processed_text = text_preprocessing(input_text, slang_dict)\n",
    "\n",
    "# Output\n",
    "print(f\"Original Text: {input_text}\")\n",
    "print(f\"Processed Text: {processed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Input Shape: (1, 100)\n",
      "Input Text: Keren banget videonya! Penjelasannya jelas dan mudah dimengerti. Ditunggu konten selanjutnya, semangat terus!\n",
      "Processed Sequence: keren banget videonya penjelasannya mudah dimengerti ditunggu konten semangat\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer.texts_to_sequences([processed_text])\n",
    "padded_input = tf.keras.preprocessing.sequence.pad_sequences(tokenized_input, maxlen=sequence_length)\n",
    "\n",
    "# Ensure the input is in the correct shape\n",
    "print(f\"Processed Input Shape: {padded_input.shape}\")\n",
    "\n",
    "# Predict using the model (make sure the model is loaded)\n",
    "predictions = model.predict(padded_input)\n",
    "\n",
    "# Check if predictions have the expected shape\n",
    "if predictions.shape[0] == 0:\n",
    "    raise ValueError(\"Model returned an empty prediction. Verify the input and model setup.\")\n",
    "\n",
    "# Interpret the prediction\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "sentiment_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "# Interpret the prediction (now converting integer to sentiment label)\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "predicted_sentiment = sentiment_mapping.get(predicted_class, \"Unknown\")  # Default to \"Unknown\" if not found\n",
    "\n",
    "# Print the result\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Processed Sequence: {processed_text}\")\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
